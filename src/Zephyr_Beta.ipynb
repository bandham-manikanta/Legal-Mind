{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e0725e9-7185-4e84-9ef3-5b67c6e9cf30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import requests\n",
    "import time\n",
    "import contextlib\n",
    "import io\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "class LegalLLMAnswerer:\n",
    "    def __init__(self, model_name: str, mode: str, hf_token: str = None, max_new_tokens: int = 300):\n",
    "        \"\"\"\n",
    "        mode: 'api' (for Zephyr) or 'local' (for local models like Mistral)\n",
    "        model_name: Hugging Face model name (e.g., HuggingFaceH4/zephyr-7b-beta)\n",
    "        hf_token: required only for API mode\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.mode = mode\n",
    "        self.max_new_tokens = max_new_tokens\n",
    "\n",
    "        if mode == \"api\":\n",
    "            assert hf_token is not None, \"Hugging Face token is required for API mode\"\n",
    "            self.api_url = f\"https://api-inference.huggingface.co/models/{model_name}\"\n",
    "            self.headers = {\n",
    "                \"Authorization\": f\"Bearer {hf_token}\",\n",
    "                \"Content-Type\": \"application/json\"\n",
    "            }\n",
    "\n",
    "        elif mode == \"local\":\n",
    "            print(f\"ðŸ” Loading local model: {model_name}\")\n",
    "            self.device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "            with contextlib.redirect_stdout(io.StringIO()):\n",
    "                self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "                self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                    model_name,\n",
    "                    device_map={\"\": self.device},\n",
    "                    torch_dtype=torch.float32\n",
    "                )\n",
    "            self.model.eval()\n",
    "            self.generator = pipeline(\n",
    "                \"text-generation\",\n",
    "                model=self.model,\n",
    "                tokenizer=self.tokenizer,\n",
    "                device=-1\n",
    "            )\n",
    "            print(f\"âœ… Model loaded on {self.device}\")\n",
    "\n",
    "    def build_prompt(self, query: str, context_docs: list) -> str:\n",
    "        context = \"\\n\\n\".join([f\"Context {i+1}:\\n{doc.strip()}\" for i, doc in enumerate(context_docs)])\n",
    "        return (\n",
    "            f\"{context}\\n\\n\"\n",
    "            f\"Question: {query.strip()}\\n\\n\"\n",
    "            f\"Instructions: Based only on the above legal context, first provide a short answer to the question, \"\n",
    "            f\"then explain your reasoning in a separate paragraph.\\n\\n\"\n",
    "            f\"Answer:\"\n",
    "        )\n",
    "\n",
    "    def generate(self, query: str, context_docs: list, debug: bool = False) -> dict:\n",
    "        prompt = self.build_prompt(query, context_docs)\n",
    "\n",
    "        if debug:\n",
    "            print(\"=\" * 80)\n",
    "            print(\"PROMPT:\\n\", prompt)\n",
    "            print(\"=\" * 80)\n",
    "\n",
    "        if self.mode == \"api\":\n",
    "            payload = {\n",
    "                \"inputs\": prompt,\n",
    "                \"parameters\": {\n",
    "                    \"max_new_tokens\": self.max_new_tokens,\n",
    "                    \"do_sample\": False,\n",
    "                    \"temperature\": 0.1,\n",
    "                    \"return_full_text\": False\n",
    "                }\n",
    "            }\n",
    "\n",
    "            response = requests.post(self.api_url, headers=self.headers, json=payload)\n",
    "            if response.status_code == 503:\n",
    "                wait = response.json().get(\"estimated_time\", 10)\n",
    "                print(f\"â³ Model warming up, retrying in {wait}s...\")\n",
    "                time.sleep(wait)\n",
    "                return self.generate(query, context_docs, debug)\n",
    "            if response.status_code != 200:\n",
    "                raise RuntimeError(f\"HF API Error {response.status_code}: {response.text}\")\n",
    "\n",
    "            text = response.json()[0][\"generated_text\"]\n",
    "\n",
    "        else:  # local\n",
    "            inputs = self.tokenizer(prompt, return_tensors=\"pt\", truncation=True, padding=True).to(self.device)\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=self.max_new_tokens,\n",
    "                    do_sample=False,\n",
    "                    temperature=0.1,\n",
    "                    pad_token_id=self.tokenizer.eos_token_id\n",
    "                )\n",
    "            text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            text = text[len(prompt):].strip()\n",
    "\n",
    "        # Parse Answer vs Reasoning\n",
    "        if \"Reasoning:\" in text:\n",
    "            answer, reasoning = text.split(\"Reasoning:\", 1)\n",
    "        else:\n",
    "            parts = text.split(\"\\n\", 1)\n",
    "            answer = parts[0]\n",
    "            reasoning = parts[1] if len(parts) > 1 else \"\"\n",
    "\n",
    "        return {\n",
    "            \"answer\": answer.strip(),\n",
    "            \"reasoning\": reasoning.strip()\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fdb662a8-f537-43ba-ac22-c3a9a5ffa2f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PROMPT:\n",
      " Context 1:\n",
      "The Supreme Court has ruled that if police have probable cause to believe a vehicle contains evidence, they may search it without a warrant.\n",
      "\n",
      "Context 2:\n",
      "This principle is known as the automobile exception to the Fourth Amendment.\n",
      "\n",
      "Context 3:\n",
      "The scope of such searches is limited to areas where the evidence might reasonably be found.\n",
      "\n",
      "Context 4:\n",
      "Searches must still meet the standard of reasonableness under the Constitution.\n",
      "\n",
      "Question: Under what conditions can police conduct a warrantless vehicle search?\n",
      "\n",
      "Instructions: Based only on the above legal context, first provide a short answer to the question, then explain your reasoning in a separate paragraph.\n",
      "\n",
      "Answer:\n",
      "================================================================================\n",
      "\n",
      "ðŸ§  Answer: Police can conduct a warrantless vehicle search if they have probable cause to believe that the vehicle contains evidence. This principle, known as the automobile exception to the Fourth Amendment, allows for searches of areas where evidence might reasonably be found, as long as the searches meet the standard of reasonableness under the Constitution.\n",
      "ðŸ“š Reasoning: The automobile exception to the Fourth Amendment recognizes that vehicles are mobile and can quickly move evidence out of police reach. As a result, the courts have determined that police should be able to search vehicles without a warrant if they have probable cause to believe that evidence is present. This exception is limited, however, to areas where evidence might reasonably be found. This means that police cannot search the entire vehicle without a warrant, but rather must limit their search to specific areas where evidence is likely to be located. Additionally, all searches, whether with or without a warrant, must still meet the standard of reasonableness under the Constitution. This means that the search must be conducted in a manner that is not excessively intrusive or violates an individual's Fourth Amendment rights.\n"
     ]
    }
   ],
   "source": [
    "hf_token = \"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\"\n",
    "\n",
    "llm = LegalLLMAnswerer(\n",
    "    model_name=\"HuggingFaceH4/zephyr-7b-beta\",\n",
    "    mode=\"api\",\n",
    "    hf_token=hf_token\n",
    ")\n",
    "\n",
    "query = \"Under what conditions can police conduct a warrantless vehicle search?\"\n",
    "\n",
    "context_docs = [\n",
    "    \"The Supreme Court has ruled that if police have probable cause to believe a vehicle contains evidence, they may search it without a warrant.\",\n",
    "    \"This principle is known as the automobile exception to the Fourth Amendment.\",\n",
    "    \"The scope of such searches is limited to areas where the evidence might reasonably be found.\",\n",
    "    \"Searches must still meet the standard of reasonableness under the Constitution.\"\n",
    "]\n",
    "\n",
    "response = llm.generate(query, context_docs, debug=True)\n",
    "\n",
    "print(\"\\nðŸ§  Answer:\", response[\"answer\"])\n",
    "print(\"ðŸ“š Reasoning:\", response[\"reasoning\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63c31e0-f88f-424a-9b62-3b51b88628c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
