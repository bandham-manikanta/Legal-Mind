{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b380ac83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bandham/miniconda3/envs/llm692_venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading corpus from legal_dummy_corpus.json...\n",
      "Loading queries from legal_sample_queries.json...\n",
      "Corpus has 120 documents\n",
      "Query set has 15 queries\n",
      "Building new dense index...\n",
      "Building dense index with 120 documents...\n",
      "Loading tokenizer for nlpaueb/legal-bert-base-uncased...\n",
      "Loading model nlpaueb/legal-bert-base-uncased...\n",
      "Generating document embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing document batches: 100%|██████████| 15/15 [00:39<00:00,  2.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building FAISS index with 768 dimensions...\n",
      "Saving index to /home/bandham/Documents/StonyBrook_CourseWork/Spring 2025/LLM-AMS692.02/Legal-Mind/legal_dense_retr2...\n",
      "Dense index built successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing queries: 100%|██████████| 15/15 [00:36<00:00,  2.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved retrieval results to dense_retrieval_results.json\n",
      "\n",
      "Sample Retrieval Results:\n",
      "========================\n",
      "\n",
      "Query: What are the essential elements of a valid contract?\n",
      "--------------------------------------------------------------------------------\n",
      "Document 1: (Score: 0.9432)\n",
      "ID: criminal_law_030\n",
      "Text: SEARCH WARRANT application states that probable cause exists to believe evidence of possession with intent to distribute will be found at Northern University Campus based on DNA analysis observed by O...\n",
      "----------------------------------------\n",
      "Document 2: (Score: 0.9333)\n",
      "ID: administrative_law_018\n",
      "Text: AGENCY DECISION: Securities and Exchange Commission hereby approves/denies {party}'s application for operating license based on findings that the party demonstrated financial responsibility. This deci...\n",
      "----------------------------------------\n",
      "\n",
      "Query: How is negligence defined in tort law?\n",
      "--------------------------------------------------------------------------------\n",
      "Document 1: (Score: 0.5427)\n",
      "ID: contract_law_028\n",
      "Text: EMPLOYMENT CONTRACT: Smith Corp. agrees to employ Commonwealth of Jefferson as Chief Financial Officer commencing on September 9, 2022 for three years. Compensation shall be $300,000 per annum with be...\n",
      "----------------------------------------\n",
      "Document 2: (Score: 0.4883)\n",
      "ID: property_law_092\n",
      "Text: EASEMENT: MediCorp grants to PacificRoute Services a perpetual easement for conservation over the property described as a 20-foot wide strip along the western edge of the property. This easement shall...\n",
      "----------------------------------------\n",
      "\n",
      "Query: What constitutes probable cause for a search warrant?\n",
      "--------------------------------------------------------------------------------\n",
      "Document 1: (Score: 0.5912)\n",
      "ID: contract_law_028\n",
      "Text: EMPLOYMENT CONTRACT: Smith Corp. agrees to employ Commonwealth of Jefferson as Chief Financial Officer commencing on September 9, 2022 for three years. Compensation shall be $300,000 per annum with be...\n",
      "----------------------------------------\n",
      "Document 2: (Score: 0.5856)\n",
      "ID: criminal_law_013\n",
      "Text: INDICTMENT: The Grand Jury charges that on July 8, 2022, defendant Richard Taylor did knowingly and intentionally misrepresented material facts, constituting the offense of money laundering under §9.2...\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import faiss\n",
    "from huggingface_hub import login\n",
    "from config import config\n",
    "login(config['HF_API_KEY']) \n",
    "\n",
    "class DenseRetriever:\n",
    "    \"\"\"\n",
    "    Dense retriever implementation using a pre-trained language model\n",
    "    for encoding documents and queries into vector representations.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_name=\"nlpaueb/legal-bert-base-uncased\", index_name=\"legal_dense_index\"):\n",
    "        \"\"\"\n",
    "        Initialize the dense retriever with a pre-trained language model.\n",
    "        \n",
    "        Args:\n",
    "            model_name: Name of the pre-trained model to use for encoding\n",
    "            index_name: Name for the index directory\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.index_name = index_name\n",
    "        self.index_dir = os.path.join(os.getcwd(), index_name)\n",
    "        os.makedirs(self.index_dir, exist_ok=True)\n",
    "        \n",
    "        # Initialize model and tokenizer\n",
    "        self.tokenizer = None\n",
    "        self.model = None\n",
    "        self.index = None\n",
    "        self.documents = None\n",
    "        self.doc_ids = None\n",
    "        self.embedding_dim = None\n",
    "        \n",
    "    def _initialize_model(self):\n",
    "        \"\"\"Load the model and tokenizer if not already loaded\"\"\"\n",
    "        if self.tokenizer is None:\n",
    "            print(f\"Loading tokenizer for {self.model_name}...\")\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "            \n",
    "        if self.model is None:\n",
    "            print(f\"Loading model {self.model_name}...\")\n",
    "            self.model = AutoModel.from_pretrained(self.model_name)\n",
    "            self.model.eval()  # Set to evaluation mode\n",
    "    \n",
    "    def get_embedding(self, text, max_length=512):\n",
    "        \"\"\"Generate embedding for a single text\"\"\"\n",
    "        # Ensure model and tokenizer are loaded\n",
    "        self._initialize_model()\n",
    "        \n",
    "        # Tokenize input\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\", max_length=max_length, \n",
    "                              padding=\"max_length\", truncation=True)\n",
    "        \n",
    "        # Generate embeddings\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "            # Use CLS token embedding as text representation\n",
    "            embedding = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "            \n",
    "        return embedding[0]  # Return as 1D array\n",
    "    \n",
    "    def index_corpus(self, documents, doc_ids, batch_size=8):\n",
    "        \"\"\"\n",
    "        Generate embeddings for all documents and build a search index.\n",
    "        \n",
    "        Args:\n",
    "            documents: List of document texts\n",
    "            doc_ids: List of document IDs corresponding to the documents\n",
    "            batch_size: Batch size for processing documents\n",
    "        \"\"\"\n",
    "        print(f\"Building dense index with {len(documents)} documents...\")\n",
    "        \n",
    "        # Store document texts and IDs\n",
    "        self.documents = documents\n",
    "        self.doc_ids = doc_ids\n",
    "        \n",
    "        # Ensure model and tokenizer are loaded\n",
    "        self._initialize_model()\n",
    "        \n",
    "        # Generate embeddings for all documents\n",
    "        print(\"Generating document embeddings...\")\n",
    "        all_embeddings = []\n",
    "        \n",
    "        for i in tqdm(range(0, len(documents), batch_size), desc=\"Processing document batches\"):\n",
    "            batch_docs = documents[i:i+batch_size]\n",
    "            batch_inputs = self.tokenizer(batch_docs, padding=True, truncation=True, \n",
    "                                         return_tensors=\"pt\", max_length=512)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**batch_inputs)\n",
    "                # Use CLS token embedding\n",
    "                batch_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "                all_embeddings.append(batch_embeddings)\n",
    "                \n",
    "        # Concatenate all batch embeddings\n",
    "        document_embeddings = np.vstack(all_embeddings)\n",
    "        self.embedding_dim = document_embeddings.shape[1]\n",
    "        \n",
    "        # Normalize embeddings for cosine similarity\n",
    "        faiss.normalize_L2(document_embeddings)\n",
    "        \n",
    "        # Build FAISS index for fast similarity search\n",
    "        print(f\"Building FAISS index with {document_embeddings.shape[1]} dimensions...\")\n",
    "        self.index = faiss.IndexFlatIP(document_embeddings.shape[1])  # Inner product for cosine similarity\n",
    "        self.index.add(document_embeddings)\n",
    "        \n",
    "        # Save the index and metadata\n",
    "        self.save_index()\n",
    "        \n",
    "        print(\"Dense index built successfully\")\n",
    "        return self\n",
    "    \n",
    "    def save_index(self):\n",
    "        \"\"\"Save the index and associated data to disk\"\"\"\n",
    "        print(f\"Saving index to {self.index_dir}...\")\n",
    "        \n",
    "        # Save FAISS index\n",
    "        if self.index is not None:\n",
    "            faiss.write_index(self.index, os.path.join(self.index_dir, \"faiss.index\"))\n",
    "        \n",
    "        # Save metadata\n",
    "        metadata = {\n",
    "            \"model_name\": self.model_name,\n",
    "            \"embedding_dim\": self.embedding_dim,\n",
    "            \"num_documents\": len(self.documents) if self.documents else 0\n",
    "        }\n",
    "        \n",
    "        with open(os.path.join(self.index_dir, \"metadata.json\"), 'w') as f:\n",
    "            json.dump(metadata, f, indent=2)\n",
    "        \n",
    "        # Save documents and doc_ids\n",
    "        with open(os.path.join(self.index_dir, \"documents.json\"), 'w') as f:\n",
    "            json.dump(self.documents, f)\n",
    "            \n",
    "        with open(os.path.join(self.index_dir, \"doc_ids.json\"), 'w') as f:\n",
    "            json.dump(self.doc_ids, f)\n",
    "    \n",
    "    def load_index(self):\n",
    "        \"\"\"Load pre-built index and associated data\"\"\"\n",
    "        index_path = os.path.join(self.index_dir, \"faiss.index\")\n",
    "        if not os.path.exists(index_path):\n",
    "            raise ValueError(f\"Index not found at {index_path}. Build index first with index_corpus()\")\n",
    "        \n",
    "        print(f\"Loading index from {self.index_dir}...\")\n",
    "        \n",
    "        # Load FAISS index\n",
    "        self.index = faiss.read_index(index_path)\n",
    "        \n",
    "        # Load metadata\n",
    "        with open(os.path.join(self.index_dir, \"metadata.json\"), 'r') as f:\n",
    "            metadata = json.load(f)\n",
    "            self.model_name = metadata[\"model_name\"]\n",
    "            self.embedding_dim = metadata[\"embedding_dim\"]\n",
    "        \n",
    "        # Load documents and doc_ids\n",
    "        with open(os.path.join(self.index_dir, \"documents.json\"), 'r') as f:\n",
    "            self.documents = json.load(f)\n",
    "            \n",
    "        with open(os.path.join(self.index_dir, \"doc_ids.json\"), 'r') as f:\n",
    "            self.doc_ids = json.load(f)\n",
    "            \n",
    "        print(f\"Loaded dense index with {len(self.documents)} documents\")\n",
    "        return self\n",
    "    \n",
    "    def retrieve(self, query, k=100):\n",
    "        \"\"\"\n",
    "        Retrieve top-k documents for a query.\n",
    "        \n",
    "        Args:\n",
    "            query: Query text\n",
    "            k: Number of documents to retrieve\n",
    "            \n",
    "        Returns:\n",
    "            List of dictionaries with document ID, score, and text\n",
    "        \"\"\"\n",
    "        if self.index is None:\n",
    "            self.load_index()\n",
    "        \n",
    "        # Generate query embedding\n",
    "        query_embedding = self.get_embedding(query)\n",
    "        query_embedding = query_embedding.reshape(1, -1)\n",
    "        \n",
    "        # Normalize query embedding for cosine similarity\n",
    "        faiss.normalize_L2(query_embedding)\n",
    "        \n",
    "        # Search index\n",
    "        scores, indices = self.index.search(query_embedding, k)\n",
    "        \n",
    "        # Format results\n",
    "        results = []\n",
    "        for i, idx in enumerate(indices[0]):\n",
    "            if idx < len(self.documents):  # Safety check\n",
    "                results.append({\n",
    "                    \"id\": self.doc_ids[idx],\n",
    "                    \"score\": float(scores[0][i]),\n",
    "                    \"text\": self.documents[idx]\n",
    "                })\n",
    "                \n",
    "        return results\n",
    "    \n",
    "    def batch_retrieve(self, queries, k=100):\n",
    "        \"\"\"\n",
    "        Retrieve top-k documents for multiple queries.\n",
    "        \n",
    "        Args:\n",
    "            queries: List of query texts\n",
    "            k: Number of documents to retrieve per query\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary mapping query index to list of results\n",
    "        \"\"\"\n",
    "        if self.index is None:\n",
    "            self.load_index()\n",
    "            \n",
    "        all_results = {}\n",
    "        for i, query in enumerate(tqdm(queries, desc=\"Processing queries\")):\n",
    "            all_results[str(i)] = self.retrieve(query, k=k)\n",
    "            \n",
    "        return all_results\n",
    "    \n",
    "    def save_results(self, results, output_file):\n",
    "        \"\"\"Save retrieval results to file\"\"\"\n",
    "        with open(output_file, 'w') as f:\n",
    "            json.dump(results, f, indent=2)\n",
    "        print(f\"Saved retrieval results to {output_file}\")\n",
    "\n",
    "# %%\n",
    "# Usage example with your generated corpus\n",
    "if __name__ == \"__main__\":\n",
    "    # Load the generated corpus\n",
    "    corpus_file = \"legal_dummy_corpus.json\"\n",
    "    queries_file = \"legal_sample_queries.json\"\n",
    "    \n",
    "    # Check if corpus file exists\n",
    "    if not os.path.exists(corpus_file):\n",
    "        print(\"Corpus file not found. Please generate the corpus first.\")\n",
    "        exit(1)\n",
    "    else:\n",
    "        # Load existing corpus\n",
    "        print(f\"Loading corpus from {corpus_file}...\")\n",
    "        with open(corpus_file, 'r') as f:\n",
    "            corpus_data = json.load(f)\n",
    "            documents = corpus_data[\"documents\"]\n",
    "            doc_ids = corpus_data[\"doc_ids\"]\n",
    "            \n",
    "    # Check if queries file exists\n",
    "    if not os.path.exists(queries_file):\n",
    "        print(\"Queries file not found. Please generate the queries first.\")\n",
    "        exit(1)\n",
    "    else:\n",
    "        # Load existing queries\n",
    "        print(f\"Loading queries from {queries_file}...\")\n",
    "        with open(queries_file, 'r') as f:\n",
    "            queries = json.load(f)\n",
    "    \n",
    "    print(f\"Corpus has {len(documents)} documents\")\n",
    "    print(f\"Query set has {len(queries)} queries\")\n",
    "    \n",
    "    # Initialize Dense retriever with a legal-domain model\n",
    "    # Using nlpaueb/legal-bert-base-uncased - specialized for legal text\n",
    "    retriever = DenseRetriever(\n",
    "        model_name=\"nlpaueb/legal-bert-base-uncased\",\n",
    "        index_name=\"legal_dense_retr2\"\n",
    "    )\n",
    "    \n",
    "    # Check if index already exists\n",
    "    if os.path.exists(os.path.join(retriever.index_dir, \"faiss.index\")):\n",
    "        print(\"Dense index already exists. Loading...\")\n",
    "        retriever.load_index()\n",
    "    else:\n",
    "        print(\"Building new dense index...\")\n",
    "        retriever.index_corpus(documents, doc_ids)\n",
    "    \n",
    "    # Retrieve results for queries\n",
    "    results = retriever.batch_retrieve(queries, k=10)\n",
    "    \n",
    "    # Save results\n",
    "    output_file = \"dense_retrieval_results.json\"\n",
    "    retriever.save_results(results, output_file)\n",
    "    \n",
    "    # Print sample results\n",
    "    print(\"\\nSample Retrieval Results:\")\n",
    "    print(\"========================\")\n",
    "    \n",
    "    for i, query in enumerate(queries[:3]):  # Show results for first 3 queries\n",
    "        print(f\"\\nQuery: {query}\")\n",
    "        print(\"-\" * 80)\n",
    "        results_for_query = results[str(i)]\n",
    "        \n",
    "        for j, doc in enumerate(results_for_query[:2]):  # Show top 2 documents\n",
    "            print(f\"Document {j+1}: (Score: {doc['score']:.4f})\")\n",
    "            print(f\"ID: {doc['id']}\")\n",
    "            print(f\"Text: {doc['text'][:200]}...\" if len(doc['text']) > 200 else f\"Text: {doc['text']}\")\n",
    "            print(\"-\" * 40)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a659f0",
   "metadata": {},
   "source": [
    "## USAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f61b17dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading index from /home/bandham/Documents/StonyBrook_CourseWork/Spring 2025/LLM-AMS692.02/Legal-Mind/legal_dense_retr2...\n",
      "Loaded dense index with 120 documents\n",
      "Loading tokenizer for nlpaueb/legal-bert-base-uncased...\n",
      "Loading model nlpaueb/legal-bert-base-uncased...\n",
      "Result 1: SEARCH WARRANT application states that probable cause exists to believe evidence of possession with ... (Score: 0.9432)\n",
      "Result 2: AGENCY DECISION: Securities and Exchange Commission hereby approves/denies {party}'s application for... (Score: 0.9333)\n",
      "Result 3: PLEA AGREEMENT: Defendant Jennifer Lee, charged with criminal negligence, agrees to plead guilty to ... (Score: 0.9310)\n",
      "Result 4: In In re Wilson Estate (2022), the Court held that religious freedom protected under the Sixth Amend... (Score: 0.9307)\n",
      "Result 5: CONSTITUTIONAL ANALYSIS: The Senate Bill 247 must be subjected to heightened scrutiny under the Four... (Score: 0.9304)\n"
     ]
    }
   ],
   "source": [
    "# from dense_retriever import DenseRetriever  # Import your class\n",
    "\n",
    "retriever = DenseRetriever(\n",
    "        model_name=\"nlpaueb/legal-bert-base-uncased\",\n",
    "        index_name=\"legal_dense_retr2\"\n",
    "    )\n",
    "\n",
    "retriever.load_index()\n",
    "\n",
    "# 3. Search with any new query\n",
    "results = retriever.retrieve(\"What are the essential elements of a valid contract?\", k=5)\n",
    "\n",
    "# 4. Process the results\n",
    "for i, result in enumerate(results):\n",
    "    print(f\"Result {i+1}: {result['text'][:100]}... (Score: {result['score']:.4f})\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm692_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
