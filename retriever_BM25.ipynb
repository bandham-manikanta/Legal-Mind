{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c2e844c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 120 legal documents across 6 domains.\n",
      "Saved to legal_dummy_corpus.json\n",
      "Saved 15 sample queries to legal_sample_queries.json\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "from rank_bm25 import BM25Okapi\n",
    "from tqdm import tqdm\n",
    "\n",
    "from generate_dummy_data import generate_legal_corpus\n",
    "\n",
    "\n",
    "class SimpleBM25Retriever:\n",
    "    \"\"\"\n",
    "    A simple BM25 retriever implementation using rank_bm25 package.\n",
    "    No Java dependencies required.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, index_name=\"legal_bm25_index\"):\n",
    "        self.index_name = index_name\n",
    "        self.index_dir = os.path.join(os.getcwd(), index_name)\n",
    "        os.makedirs(self.index_dir, exist_ok=True)\n",
    "        self.bm25 = None\n",
    "        self.tokenized_corpus = None\n",
    "        self.documents = None\n",
    "        self.doc_ids = None\n",
    "    \n",
    "    def tokenize(self, text):\n",
    "        \"\"\"Simple whitespace tokenization\"\"\"\n",
    "        return text.lower().split()\n",
    "    \n",
    "    def index_corpus(self, documents, doc_ids):\n",
    "        \"\"\"Build BM25 index from documents\"\"\"\n",
    "        print(f\"Building BM25 index with {len(documents)} documents...\")\n",
    "        \n",
    "        self.documents = documents\n",
    "        self.doc_ids = doc_ids\n",
    "        \n",
    "        # Tokenize corpus\n",
    "        print(\"Tokenizing documents...\")\n",
    "        self.tokenized_corpus = [self.tokenize(doc) for doc in tqdm(documents, total=len(documents))]\n",
    "        \n",
    "        # Build BM25 index\n",
    "        print(\"Building BM25 index...\")\n",
    "        self.bm25 = BM25Okapi(self.tokenized_corpus)\n",
    "        \n",
    "        # Save the index\n",
    "        self.save_index()\n",
    "        \n",
    "        print(\"BM25 index built successfully\")\n",
    "        return self\n",
    "    \n",
    "    def save_index(self):\n",
    "        \"\"\"Save the index to disk\"\"\"\n",
    "\n",
    "        # bm25.pkl: The BM25 scoring object with term frequencies and IDF values\n",
    "        with open(os.path.join(self.index_dir, \"bm25.pkl\"), 'wb') as f:\n",
    "            pickle.dump(self.bm25, f)\n",
    "        \n",
    "        # tokenized_corpus.pkl: The tokenized versions of all documents\n",
    "        with open(os.path.join(self.index_dir, \"tokenized_corpus.pkl\"), 'wb') as f:\n",
    "            pickle.dump(self.tokenized_corpus, f)\n",
    "        \n",
    "        # documents.json: The original document texts\n",
    "        with open(os.path.join(self.index_dir, \"documents.json\"), 'w') as f:\n",
    "            json.dump(self.documents, f)\n",
    "        \n",
    "        # doc_ids.json: The document IDs\n",
    "        with open(os.path.join(self.index_dir, \"doc_ids.json\"), 'w') as f:\n",
    "            json.dump(self.doc_ids, f)\n",
    "    \n",
    "    def load_index(self):\n",
    "        \"\"\"Load pre-built BM25 index\"\"\"\n",
    "        index_path = os.path.join(self.index_dir, \"bm25.pkl\")\n",
    "        if not os.path.exists(index_path):\n",
    "            raise ValueError(f\"Index not found at {index_path}. Build index first with index_corpus()\")\n",
    "        \n",
    "        with open(index_path, 'rb') as f:\n",
    "            self.bm25 = pickle.load(f)\n",
    "        \n",
    "        with open(os.path.join(self.index_dir, \"tokenized_corpus.pkl\"), 'rb') as f:\n",
    "            self.tokenized_corpus = pickle.load(f)\n",
    "        \n",
    "        with open(os.path.join(self.index_dir, \"documents.json\"), 'r') as f:\n",
    "            self.documents = json.load(f)\n",
    "        \n",
    "        with open(os.path.join(self.index_dir, \"doc_ids.json\"), 'r') as f:\n",
    "            self.doc_ids = json.load(f)\n",
    "        \n",
    "        print(f\"Loaded BM25 index with {len(self.documents)} documents\")\n",
    "        return self\n",
    "    \n",
    "    def retrieve(self, query, k=100):\n",
    "        \"\"\"Retrieve top-k documents for a query\"\"\"\n",
    "        if self.bm25 is None:\n",
    "            self.load_index()\n",
    "        \n",
    "        # Tokenize query\n",
    "        tokenized_query = self.tokenize(query)\n",
    "        \n",
    "        # Get scores\n",
    "        scores = self.bm25.get_scores(tokenized_query)\n",
    "        \n",
    "        # Get top k document indices\n",
    "        top_indices = np.argsort(scores)[::-1][:k]\n",
    "        \n",
    "        # Format results\n",
    "        results = []\n",
    "        for i in top_indices:\n",
    "            if scores[i] > 0:  # Only include documents with non-zero scores\n",
    "                results.append({\n",
    "                    \"id\": self.doc_ids[i],\n",
    "                    \"score\": float(scores[i]),\n",
    "                    \"text\": self.documents[i]\n",
    "                })\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def batch_retrieve(self, queries, k=100):\n",
    "        \"\"\"Retrieve top-k documents for multiple queries\"\"\"\n",
    "        if self.bm25 is None:\n",
    "            self.load_index()\n",
    "        \n",
    "        all_results = {}\n",
    "        for i, query in enumerate(tqdm(queries, desc=\"Processing queries\")):\n",
    "            all_results[str(i)] = self.retrieve(query, k=k)\n",
    "        \n",
    "        return all_results\n",
    "    \n",
    "    def save_results(self, results, output_file):\n",
    "        \"\"\"Save retrieval results to file\"\"\"\n",
    "        with open(output_file, 'w') as f:\n",
    "            json.dump(results, f, indent=2)\n",
    "        print(f\"Saved retrieval results to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18de8e86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading corpus from legal_dummy_corpus.json...\n",
      "Loading queries from legal_sample_queries.json...\n",
      "Corpus has 120 documents\n",
      "Query set has 15 queries\n",
      "Index already exists. Loading...\n",
      "Loaded BM25 index with 120 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing queries: 100%|██████████| 15/15 [00:00<00:00, 1814.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved retrieval results to bm25_retrieval_results.json\n",
      "\n",
      "Sample Retrieval Results:\n",
      "========================\n",
      "\n",
      "Query: What are the essential elements of a valid contract?\n",
      "--------------------------------------------------------------------------------\n",
      "Document 1: (Score: 4.4936)\n",
      "ID: property_law_119\n",
      "Text: EASEMENT: TransAmerica Logistics grants to Commonwealth of Jefferson a perpetual easement for conservation over the property described as a 20-foot wide strip along the western edge of the property. T...\n",
      "----------------------------------------\n",
      "Document 2: (Score: 4.1827)\n",
      "ID: property_law_092\n",
      "Text: EASEMENT: MediCorp grants to PacificRoute Services a perpetual easement for conservation over the property described as a 20-foot wide strip along the western edge of the property. This easement shall...\n",
      "----------------------------------------\n",
      "\n",
      "Query: How is negligence defined in tort law?\n",
      "--------------------------------------------------------------------------------\n",
      "Document 1: (Score: 5.5100)\n",
      "ID: tort_law_073\n",
      "Text: The tort claim filed by Sarah Williams asserts that Omega Corporation's ignored industry safety standards constituted negligence and breached the duty of care owed to plaintiff, causing severe physica...\n",
      "----------------------------------------\n",
      "Document 2: (Score: 3.2071)\n",
      "ID: tort_law_083\n",
      "Text: NEGLIGENCE CLAIM: Robert Johnson alleges that on August 30, 2022, Thomas Anderson failed to {duty} resulting in emotional distress. The standard of care required defendant to exacting, which was breac...\n",
      "----------------------------------------\n",
      "\n",
      "Query: What constitutes probable cause for a search warrant?\n",
      "--------------------------------------------------------------------------------\n",
      "Document 1: (Score: 7.2187)\n",
      "ID: criminal_law_116\n",
      "Text: SEARCH WARRANT application states that probable cause exists to believe evidence of insider trading will be found at Central Park based on witness statements observed by Officer C. Wilson on March 3, ...\n",
      "----------------------------------------\n",
      "Document 2: (Score: 7.2187)\n",
      "ID: criminal_law_068\n",
      "Text: SEARCH WARRANT application states that probable cause exists to believe evidence of bribery will be found at Downtown Financial District based on digital communications observed by Officer D. Williams...\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# Modified usage to work with your generated data\n",
    "if __name__ == \"__main__\":\n",
    "    # Load the generated corpus\n",
    "    corpus_file = \"legal_dummy_corpus.json\"\n",
    "    queries_file = \"legal_sample_queries.json\"\n",
    "    \n",
    "    # Check if corpus file exists, otherwise run data generation\n",
    "    if not os.path.exists(corpus_file):\n",
    "        print(\"Corpus file not found. Generating dummy legal corpus...\")\n",
    "        corpus = generate_legal_corpus(120, corpus_file)\n",
    "        documents = corpus[\"documents\"]\n",
    "        doc_ids = corpus[\"doc_ids\"]\n",
    "    else:\n",
    "        # Load existing corpus\n",
    "        print(f\"Loading corpus from {corpus_file}...\")\n",
    "        with open(corpus_file, 'r') as f:\n",
    "            corpus_data = json.load(f)\n",
    "            documents = corpus_data[\"documents\"]\n",
    "            doc_ids = corpus_data[\"doc_ids\"]\n",
    "            \n",
    "    # Check if queries file exists\n",
    "    if not os.path.exists(queries_file):\n",
    "        print(\"Queries file not found. Using default queries...\")\n",
    "        queries = [\n",
    "            \"What are the essential elements of a valid contract?\",\n",
    "            \"How is negligence defined in tort law?\",\n",
    "            \"What constitutes probable cause for a search warrant?\",\n",
    "            \"What rights are protected under the First Amendment?\",\n",
    "            \"How does adverse possession work in property law?\",\n",
    "            \"What is the standard for proving defamation?\",\n",
    "            \"What are the remedies for breach of contract?\",\n",
    "            \"How does the Fourth Amendment limit police searches?\",\n",
    "            \"What is the process for appealing an administrative decision?\",\n",
    "            \"What constitutes insider trading under securities regulations?\",\n",
    "            \"How are easements created and terminated?\",\n",
    "            \"What is the difference between murder and manslaughter?\",\n",
    "            \"What are the requirements for a valid will?\",\n",
    "            \"How does eminent domain work?\",\n",
    "            \"What constitutes workplace discrimination?\"\n",
    "        ]\n",
    "    else:\n",
    "        # Load existing queries\n",
    "        print(f\"Loading queries from {queries_file}...\")\n",
    "        with open(queries_file, 'r') as f:\n",
    "            queries = json.load(f)\n",
    "    \n",
    "    print(f\"Corpus has {len(documents)} documents\")\n",
    "    print(f\"Query set has {len(queries)} queries\")\n",
    "    \n",
    "    # Initialize BM25 retriever\n",
    "    retriever = SimpleBM25Retriever(index_name=\"legal_bm25_retr1\")\n",
    "    \n",
    "    # Check if index already exists\n",
    "    if os.path.exists(os.path.join(retriever.index_dir, \"bm25.pkl\")):\n",
    "        print(\"Index already exists. Loading...\")\n",
    "        retriever.load_index()\n",
    "    else:\n",
    "        print(\"Building new index...\")\n",
    "        retriever.index_corpus(documents, doc_ids)\n",
    "    \n",
    "    # Retrieve results for queries\n",
    "    results = retriever.batch_retrieve(queries, k=10)\n",
    "    \n",
    "    # Save results\n",
    "    output_file = \"bm25_retrieval_results.json\"\n",
    "    retriever.save_results(results, output_file)\n",
    "    \n",
    "    # Print sample results\n",
    "    print(\"\\nSample Retrieval Results:\")\n",
    "    print(\"========================\")\n",
    "    \n",
    "    for i, query in enumerate(queries[:3]):  # Show results for first 3 queries\n",
    "        print(f\"\\nQuery: {query}\")\n",
    "        print(\"-\" * 80)\n",
    "        results_for_query = results[str(i)]\n",
    "        \n",
    "        for j, doc in enumerate(results_for_query[:2]):  # Show top 2 documents\n",
    "            print(f\"Document {j+1}: (Score: {doc['score']:.4f})\")\n",
    "            print(f\"ID: {doc['id']}\")\n",
    "            print(f\"Text: {doc['text'][:200]}...\" if len(doc['text']) > 200 else f\"Text: {doc['text']}\")\n",
    "            print(\"-\" * 40)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm692_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
